---
layout: archive
title: "Research"
permalink: /research/
author_profile: true
---
<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<style>
.collapsible {
  background-color: #949494;
  color: white;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
}

.active, .collapsible:hover {
  background-color: #d7d7d7;
}

.collapsible:after {
  content: '\002B';
  color: white;
  font-weight: bold;
  float: right;
  margin-left: 5px;
}

.active:after {
  content: "\2212";
}

.content {
  padding: 0 18px;
  background-color: #eaeaea;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}

</style>
</head>
<body>


<p>I am broadly interested in computer security and privacy problems.
  More particularly, my research studies techniques to <em>quantify the privacy leakage</em> of various systems and designs defenses to limit this leakage.
</p>
<p>
  I have written this informal page to help interested students understand what I do.
  For more technical descriptions, please check my papers.
</p>
Click on the research lines below to learn more about them.
</p>

<button type="button" class="collapsible">Machine Learning Privacy</button>
<div class="content">
  <img align="right" src="/images/topic-ml.png" alt="ML topic" width="195">
  <p>
    Machine learning technologies are everywhere.
    Here's the privacy angle of all this: when one builds a machine learning model using a privacy-sensitive training dataset, the model will memorize some privacy-sensitive information.
    An attacker with access to that model (even if it is just black-box query access) can learn a lot of information about the training data.
    The most basic attack is called <strong>membership inference attack</strong>, where an attacker gets access to a model and a data sample, and wants to get whether that sample was in the model's training set. 
    More complex attacks include model inversion, where the attacker reconstructs a training sample just by querying the model!
    Sadly, due to the complexity of machine learning (it's called <em>machine</em> learning, after all), preventing these attacks is really hard!
  </p>
  <p>
    I am interested in understanding the privacy implications of training machine learning models on sensitive data, developing tools to quantify their privacy leakage, and building defenses to limit such leakage.
    As with the rest of my research, I do not like <em>purely empirical</em> or heuristic results, and I try to get some solid understanding of why things work the way they do.
    This is sadly somewhat hard in machine learning, but we have to try!
  </p>
  <p>
    The only paper I have (so far) in the topic should give you a good idea of the type of questions I like to study in this area:
  </p>
  <h3>Publications</h3>
  <ul>{% for post in site.publications reversed %}
    {% if post.area == 'ml' %}
    <li>{% include archive-single-simon.html %}</li>
    {% endif %}
  {% endfor %}</ul>
</div>


<button type="button" class="collapsible">Privacy-Preserving Searchable Encryption</button>
<div class="content">
  <img align="right" src="/images/topic-sse.png" alt="SSE topic">
  <p>Storing data in the cloud is very convenient: it frees resources on our local devices and helps the data be available as long as we have some Internet connection.
    However, storing privacy-sensitive data in the cloud is somewhat scary: what if the cloud is looking at our data? Can we trust the cloud?
  </p>
  <p>
    Luckily, humans are very good at hiding data using encryption!
    We can encrypt of all our data and store it in the cloud.
    However, if we try to search for any data item, turns out we cannot do this by default if the data is encrypted.
    Enter <strong>searchable encryption</strong>!
  </p>
  <p>
    Searchable encryption allows storing encrypted data in a cloud provider, alongside some cryptographic structures that allow running secure searches on the data.
    Now the cloud provider does not learn anything about our data nor how we access it.
    However, total protection incurs a lot of overhead, which means that, in practice, we should leak <em>some</em> information to the cloud provider.
    Understanding the implications of such leakage and designing systems that limit such leakage (while still being efficient) are two very important questions that my research deals with.
  </p>
  <p>
    I have looked at this problem by developing attacks against searchable encryption schemes, since the performance of an attack gives us some indication of the privacy level the system provides.
    If you are curious about this, check my publications below.
    I recommend starting with the <a href="/files/oya-2021-08-usenix.pdf">SAP attack</a>, which has enough statistics to show you the kind of work I do.
    If you liked that, check a more sophisticated attack called <a href="/files/oya-2022-08-usenix.pdf">IHOP</a>, as well as my produest academic achievement: <a href="/files/oya-2022-08-usenix-slides.pdf">the slides</a> I used to present this work.
    There is also a video of that talk somewhere, but please don't watch it.
  </p>
  <h3>Publications</h3>
  <ul>{% for post in site.publications reversed %}
    {% if post.area == 'sse' %}
    <li>{% include archive-single-simon.html %}</li>
    {% endif %}
  {% endfor %}</ul>
</div>


<button type="button" class="collapsible">Anonymous Communications</button>
<div class="content">
  <img align="right" src="/images/topic-anon.png" alt="Anonymous coms topic">
  <p>
    Nowadays, most Internet communications are encrypted, which is great!
    However, even if your <em>data</em> is encrypted, it is not hard for an adversary to tell <em>where</em> your traffic is going (e.g., that particular website you are visiting).
    To hide who talks with whom over the Internet (and other <em>meta-data</em>), we need <strong>anonymous communications systems</strong>.
    <a href="https://www.torproject.org/">Tor</a> is the most successful of such systems, but there is another type of systems, less known, called <a href="https://en.wikipedia.org/wiki/Mix_network">mixnets</a>.
    Mixnets aim to protect against a global passive adversary, which is something that even Tor cannot do.
    While mixnets were conceived a long time ago, their deployments are very recent.
  </p>
  <p>
    My research in mixnets focuses on understanding and quantifying the privacy guarantees that they provide.
    In the past, I have done this from a theoretical perspective, using statistics and signal processing techniques.
    Recently, we have seen some actual deployments of mixnets (e.g., <a href="https://nymtech.net/about/mixnet">Nym</a>), and I would love to study the privacy of these specific systems!
  </p>
  <p>Below are my papers on the topic.
    If you are a student looking for one paper to read on this topic, start with <a href="/files/oya-2013-12-globalsip.pdf">my first paper ever</a> (only four pages, and enough math to scare you off).
    If you enjoyed that and want to read more math, also give <a href="/files/oya-2014-07-pets.pdf">this one</a> a read.
  </p>
  <h3>Publications</h3>
  <ul>{% for post in site.publications reversed %}
    {% if post.area == 'anon' %}
    <li>{% include archive-single-simon.html %}</li>
    {% endif %}
  {% endfor %}</ul>
</div>


<button type="button" class="collapsible">Location Privacy</button>
<div class="content">
  <img align="right" src="/images/topic-location.png" alt="Location privacy topic">
  <p>Open your phone, got to location permissions, and see how many apps you are sharing your location with.
    Probably a lot.
    Food delivery, rideshare, weather forecast, camera, dating apps, etc.
    To use most of these apps, we <em>need to share our location data</em> with a service provider.
    Have you ever wondered how sensitive this information is?
    With your location information is not only trivial to learn where you live and work, but one could probably infer your hobbies, habits, political preferences, etc.
    What if the service provider is malicious or does not have the proper safeguards to protect your data?
    Scary!
  </p>
  <p>
    My research in location privacy studies <strong>location privacy-preserving mechanisms</strong>, which partially hide our location to provide some privacy while still being able to get some utility from the service.
    My work so far has focused on reasoning about which metrics are appropriate to quantify privacy in this case, and studied how to optimize the privacy-utility trade-offs in this setting.
  </p>
  <p>
    If you want to read one paper about this, I would recommend my <a href="/files/oya-2017-11-ccs.pdf">CCS'17 paper</a>.
    Here's the full list:
  </p>
  <h3>Publications</h3>
  <ul>{% for post in site.publications reversed %}
    {% if post.area == 'loc' %}
    <li>{% include archive-single-simon.html %}</li>
    {% endif %}
  {% endfor %}</ul>
</div>


<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    }
  });
}
</script>

Besides this, I would be open to explore other privacy problems where my knowledge can be helpful.
Basically: if there is a technology that relies on data collected from people, this data can be privacy-sensitive, and we do not know how to measure or prevent the privacy leakage, then that is potentially a problem I want to work on.
I also love doing work that involves <em>differential privacy</em> in some way!
</body>
</html>
